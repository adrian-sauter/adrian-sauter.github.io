<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Visually Grounded Text Embeddings | Adrian Sauter </title> <meta name="author" content="Adrian Sauter"> <meta name="description" content="Short project on the effects of visual grounding on the semanticity of text embeddings."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8A&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adrian-sauter.github.io/projects/visually_grounded_text_embeddings/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Adrian</span> Sauter </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/coursework/">coursework </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Adrian_Sauter_CV.pdf" target="_blank">curriculum vitae</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Visually Grounded Text Embeddings</h1> <p class="post-description">Short project on the effects of visual grounding on the semanticity of text embeddings.</p> </header> <article> <p> <strong>TL;DR:</strong> This mini-project was done in 4 days as part of the [Interpretability and Explainability in AI](https://coursecatalogue.uva.nl/xmlpages/page/2023-2024-en/search-course/course/110133) and earned my group the "best project award" out of ~30 projects. </p> <div class="text-center my-4"> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/visually_grounded_text/paper_info-480.webp 480w,/assets/img/projects/visually_grounded_text/paper_info-800.webp 800w,/assets/img/projects/visually_grounded_text/paper_info-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/visually_grounded_text/paper_info.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="VG-BERT Overview" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption mt-2"> Overview over different types of embeddings. Classic word embeddings are learned through text only. VG-BERT combines a language stream and a visual stream to obtain visually grounded word embeddings. The bilinear relation module further enhances these embeddings, resulting in relationally grounded word embeddings. </div> </div> <div class="row my-4"> <div class="col"> <h3>Background</h3> <p> This project is based on the paper "Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning" <a class="citation" href="#zhang2021explainable">(Zhang et al., 2021)</a>. The authors introduce VG-BERT, a vision-language model, which grounds language learning in vision, which is inspired by the fact that humans learn language by grounding concepts in perception and action and follows other research on vision-language models (e.g. CLIP <a class="citation" href="#radford2021learning">(Radford et al., 2021)</a>). The model consists of a visual stream, based on a VGG model <a class="citation" href="#simonyan2014very">(Simonyan &amp; Zisserman, 2014)</a>, and a language stream, based on a BERT-model <a class="citation" href="#devlin2019bert">(Devlin et al., 2019)</a>. The model learns to align visual and language representations via cross-modal contrastive learning. After training, the language stream is a stand-alone language model capable of embedding concepts in a visually grounded semantic space. For more details on the architecture of VG-BERT, refer to the original paper <a class="citation" href="#zhang2021explainable">(Zhang et al., 2021)</a>. </p> </div> </div> <div class="row my-4"> <div class="col"> <h3>Research Questions</h3> <p> Inspired by the findings that VG-BERT's representational space is semantically more meaningful compared to BERT's representatinal space <a class="citation" href="#zhang2021explainable">(Zhang et al., 2021)</a>, we pose the following research questions to further examine the impact of visual grounding: </p> <ul> <li>Does visual grounding equally impact concrete and abstract concepts?</li> <li>Does visual grounding help with resolving lexical ambiguities?</li> <li>Are the visually grounded embeddings clustered better?</li> <li>How does VG-BERT compare to BERT on probing tasks?</li> </ul> </div> </div> <div class="row my-4"> <div class="col"> <div class="text-center my-4"> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/visually_grounded_text/abstract_concrete-480.webp 480w,/assets/img/projects/visually_grounded_text/abstract_concrete-800.webp 800w,/assets/img/projects/visually_grounded_text/abstract_concrete-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/projects/visually_grounded_text/abstract_concrete.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Abstract vs Concrete words" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption mt-2"> Cosine similarities abstract vs concrete (between models) </div> </div> <p> Concrete concepts are We defined a set of abstract and concrete words and </p> </div> </div> <p>Testing the disambiguation abilities</p> <hr> <div class="row my-4"> <div class="col"> <h3>Conclusion</h3> <p> Grounding language models in vision enhances their treatment of concrete concepts, disambiguates homonyms, and improves semantic clustering, without disrupting their syntactic representations. </p> </div> </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="zhang2021explainable" class="col-sm-8"> <div class="title">Explainable semantic space by grounding language to vision with cross-modal contrastive learning</div> <div class="author"> Yizhen Zhang, Minkyu Choi, Kuan Han, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Zhongming Liu' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2021explainable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Explainable semantic space by grounding language to vision with cross-modal contrastive learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yizhen and Choi, Minkyu and Han, Kuan and Liu, Zhongming}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{18513--18526}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="radford2021learning" class="col-sm-8"> <div class="title">Learning transferable visual models from natural language supervision</div> <div class="author"> Alec Radford, Jong Wook Kim, Chris Hallacy, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>In International conference on machine learning</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">radford2021learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning transferable visual models from natural language supervision}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International conference on machine learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8748--8763}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PmLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL</abbr> </div> <div id="devlin2019bert" class="col-sm-8"> <div class="title">Bert: Pre-training of deep bidirectional transformers for language understanding</div> <div class="author"> Jacob Devlin, Ming-Wei Chang, Kenton Lee, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Kristina Toutanova' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">devlin2019bert</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bert: Pre-training of deep bidirectional transformers for language understanding}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4171--4186}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="simonyan2014very" class="col-sm-8"> <div class="title">Very deep convolutional networks for large-scale image recognition</div> <div class="author"> Karen Simonyan, and Andrew Zisserman </div> <div class="periodical"> <em>arXiv preprint arXiv:1409.1556</em>, 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">simonyan2014very</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Very deep convolutional networks for large-scale image recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Simonyan, Karen and Zisserman, Andrew}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:1409.1556}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Adrian Sauter. Last updated: October 02, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script src="/assets/js/bibsearch.js?a8796296a5e2f80c5e498cdd51d7761e" type="module"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"projects",description:"Non-exhaustive list of projects I have worked on in the past. More projects and details will be added soon!",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-coursework",title:"coursework",description:"",section:"Navigation",handler:()=>{window.location.href="/coursework/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-github-metadata",title:"a post with github metadata",description:"a quick run down on accessing github metadata.",section:"Posts",handler:()=>{window.location.href="/blog/2020/github-metadata/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-i-started-my-msc-in-artificial-intelligence-at-the-university-of-amsterdam",title:"I started my MSc in Artificial Intelligence at the University of Amsterdam.",description:"",section:"News"},{id:"news-our-project-exploring-visually-grounded-word-embeddings-poster-has-won-the-best-project-award-in-the-interpretability-and-explainability-in-ai-course",title:"Our project \u201cExploring Visually Grounded Word Embeddings\u201d (poster) has won the best-project award...",description:"",section:"News"},{id:"news-i-began-my-second-year-in-the-msc-artificial-intelligence-program-at-the-university-of-amsterdam-and-took-up-a-position-as-a-teaching-assistant-for-the-courses-computer-vision-1-and-natural-language-processing-1-taught-to-first-year-msc-ai-students",title:"I began my second year in the MSc Artificial Intelligence program at the...",description:"",section:"News"},{id:"news-our-paper-studying-how-to-efficiently-and-effectively-guide-models-with-explanations-a-reproducibility-study-has-been-accepted-at-the-ml-reproducibility-challenge-2023-you-can-find-it-here-on-openreview",title:"Our paper \u201c\u201cStudying How to Efficiently and Effectively Guide Models with Explanations\u201d -...",description:"",section:"News"},{id:"news-i-m-excited-to-announce-that-my-co-authors-and-i-will-be-presenting-our-recent-paper-studying-how-to-efficiently-and-effectively-guide-models-with-explanations-a-reproducibility-study-at-neurips-in-vancouver-this-december",title:"I\u2019m excited to announce that my co-authors and I will be presenting our...",description:"",section:"News"},{id:"news-i-recently-started-a-role-as-an-xai-intern-at-kpn-in-amsterdam-over-the-next-six-months-i-ll-be-researching-explainable-customer-call-classification-applying-concept-bottleneck-models-to-text-data-with-the-use-of-llms",title:"I recently started a role as an XAI intern at KPN in Amsterdam....",description:"",section:"News"},{id:"news-today-i-started-the-third-year-of-my-ai-master-s-at-the-university-of-amsterdam-over-the-next-six-months-i-ll-be-working-on-my-thesis-contextual-sensitivity-in-moral-judgements-of-large-language-models-while-continuing-my-role-as-a-teaching-assistant-for-first-year-master-s-students",title:"Today, I started the third year of my AI Master\u2019s at the University...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"projects-visually-grounded-text-embeddings",title:"Visually Grounded Text Embeddings",description:"Short project on the effects of visual grounding on the semanticity of text embeddings.",section:"Projects",handler:()=>{window.location.href="/projects/visually_grounded_text_embeddings/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%64%72%69%61%6E%73%61%75%74%65%72%30%37.%61%73@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=EaGxyFAAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/adrian-sauter","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/adrian-sauter-ai","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>